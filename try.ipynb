{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import wikipedia\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "# Multilingual embedding model (XLM-Roberta-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Function to embed text\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to extract Wikidata information\n",
    "def query_wikidata(qid):\n",
    "    endpoint = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    query = f\"\"\"\n",
    "    SELECT ?itemLabel ?itemDescription ?instanceOfLabel ?countryOfOriginLabel ?cultureLabel ?mainSubjectLabel WHERE {{\n",
    "        wd:{qid} rdfs:label ?itemLabel.\n",
    "        OPTIONAL {{ wd:{qid} schema:description ?itemDescription FILTER(LANG(?itemDescription) = \"en\"). }}\n",
    "        OPTIONAL {{ wd:{qid} wdt:P31 ?instanceOf. ?instanceOf rdfs:label ?instanceOfLabel FILTER(LANG(?instanceOfLabel) = \"en\"). }}\n",
    "        OPTIONAL {{ wd:{qid} wdt:P495 ?countryOfOrigin. ?countryOfOrigin rdfs:label ?countryOfOriginLabel FILTER(LANG(?countryOfOriginLabel) = \"en\"). }}\n",
    "        OPTIONAL {{ wd:{qid} wdt:P2596 ?culture. ?culture rdfs:label ?cultureLabel FILTER(LANG(?cultureLabel) = \"en\"). }}\n",
    "        OPTIONAL {{ wd:{qid} wdt:P921 ?mainSubject. ?mainSubject rdfs:label ?mainSubjectLabel FILTER(LANG(?mainSubjectLabel) = \"en\"). }}\n",
    "        FILTER(LANG(?itemLabel) = \"en\")\n",
    "    }} LIMIT 1\n",
    "    \"\"\"\n",
    "    endpoint.setQuery(query)\n",
    "    endpoint.setReturnFormat(JSON)\n",
    "    results = endpoint.query().convert()\n",
    "    return results['results']['bindings'][0]\n",
    "\n",
    "# Function to get Wikipedia content and categories\n",
    "def get_wikipedia_data(name):\n",
    "    try:\n",
    "        page = wikipedia.page(name)\n",
    "        content = page.content[:1000]  # first 1000 chars as semantic embedding input\n",
    "        categories = page.categories\n",
    "        lang_count = len(page.langlinks)  # number of language variants\n",
    "        return content, categories, lang_count\n",
    "    except Exception as e:\n",
    "        print(f\"Wikipedia error for {name}: {e}\")\n",
    "        return \"\", [], 0\n",
    "\n",
    "# Example dataset row (Wikidata ID: Q811389)\n",
    "qid = \"Q811389\"  # Bauhaus Archive\n",
    "\n",
    "# Extract Wikidata data\n",
    "wikidata_info = query_wikidata(qid)\n",
    "\n",
    "item_name = wikidata_info['itemLabel']['value']\n",
    "description = wikidata_info.get('itemDescription', {}).get('value', '')\n",
    "instance_of = wikidata_info.get('instanceOfLabel', {}).get('value', '')\n",
    "country_origin = wikidata_info.get('countryOfOriginLabel', {}).get('value', '')\n",
    "culture = wikidata_info.get('cultureLabel', {}).get('value', '')\n",
    "main_subject = wikidata_info.get('mainSubjectLabel', {}).get('value', '')\n",
    "\n",
    "# Print Wikidata features\n",
    "print(f\"Item Name: {item_name}\")\n",
    "print(f\"Description: {description}\")\n",
    "print(f\"Instance of: {instance_of}\")\n",
    "print(f\"Country of Origin: {country_origin}\")\n",
    "print(f\"Culture: {culture}\")\n",
    "print(f\"Main Subject: {main_subject}\")\n",
    "\n",
    "# Extract Wikipedia data\n",
    "wiki_content, wiki_categories, lang_variants = get_wikipedia_data(item_name)\n",
    "\n",
    "print(f\"Categories: {wiki_categories}\")\n",
    "print(f\"Language Variants: {lang_variants}\")\n",
    "\n",
    "# Embed Wikipedia textual data\n",
    "embedding = get_embedding(wiki_content)\n",
    "\n",
    "# Combine features\n",
    "features = {\n",
    "    \"name\": item_name,\n",
    "    \"description\": description,\n",
    "    \"instance_of\": instance_of,\n",
    "    \"country_origin\": country_origin,\n",
    "    \"culture\": culture,\n",
    "    \"main_subject\": main_subject,\n",
    "    \"categories\": wiki_categories,\n",
    "    \"language_variants\": lang_variants,\n",
    "    \"embedding\": embedding\n",
    "}\n",
    "\n",
    "# Build Graph with NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add main node\n",
    "G.add_node(qid, **features)\n",
    "\n",
    "# Example: Adding semantic edges (to country, main subject, instance_of)\n",
    "semantic_relations = {\n",
    "    'country_origin': country_origin,\n",
    "    'main_subject': main_subject,\n",
    "    'instance_of': instance_of\n",
    "}\n",
    "\n",
    "# Add nodes and edges based on relations\n",
    "for relation, entity in semantic_relations.items():\n",
    "    if entity:\n",
    "        # Add entity node (simplified; in practice extract further features for these nodes)\n",
    "        entity_node_id = f\"{relation}_{entity.replace(' ', '_')}\"\n",
    "        G.add_node(entity_node_id, name=entity, type=relation)\n",
    "\n",
    "        # Add edge from main node to entity node\n",
    "        G.add_edge(qid, entity_node_id, relation=relation)\n",
    "\n",
    "# Visualization (optional sanity check)\n",
    "print(f\"Nodes: {G.nodes(data=True)}\")\n",
    "print(f\"Edges: {G.edges(data=True)}\")\n",
    "\n",
    "# Now 'G' contains structured nodes with features and semantic edges\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
